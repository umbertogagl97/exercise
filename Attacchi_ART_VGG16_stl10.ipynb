{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Attacchi ART VGG16_stl10.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "nGmEBN4oxqUO",
        "-ycpjmyb4Cxe",
        "dherpFOH0Uau",
        "5X10jetEyAax",
        "9XepXK-ny3XN",
        "l8z4jggt3ZUG",
        "tFFWkmjX2j5P",
        "VjnPQ4NtzzQH",
        "_UByTbhH01i3",
        "4X4ZaKM8z9hk",
        "Suibji7V0I7j",
        "nH0_wJWt2XuF",
        "nLcnge6A0Npi",
        "nkdhgkab0e-B",
        "fHJP8DBX0k90"
      ],
      "mount_file_id": "1LFvCjg8zmdzf9GGORH3M3sLCejtQ0qUI",
      "authorship_tag": "ABX9TyOIhL9YIW6eiVq2T386N0WV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/umbertogagl97/exercise/blob/main/Attacchi_ART_VGG16_stl10.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nGmEBN4oxqUO"
      },
      "source": [
        "# **Import iniziali**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OHENSlCXxzcr"
      },
      "source": [
        "Import ART"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hIKN5Oqa-i6u"
      },
      "source": [
        "#importa ART\n",
        "!pip install adversarial-robustness-toolbox"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AHA120xNx0-M"
      },
      "source": [
        "Librerie"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "01RXI-DDIb3C"
      },
      "source": [
        "#Librerie\n",
        "import tensorflow as tf\n",
        "\n",
        "tf.compat.v1.disable_eager_execution()\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPooling2D, Activation, Dropout\n",
        "from tensorflow.keras.losses import categorical_crossentropy\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import numpy as np\n",
        "\n",
        "from art.attacks.evasion import FastGradientMethod\n",
        "from art.estimators.classification import KerasClassifier\n",
        "from art.utils import load_dataset\n",
        "\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "#import torch"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ycpjmyb4Cxe"
      },
      "source": [
        "#**Def variabili**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4hMuySVI4GgY"
      },
      "source": [
        "#salvataggio modello\n",
        "model_save_name = 'model_prova'\n",
        "path_model_save = F\"/content/gdrive/My Drive/ModelliCNN/{model_save_name}\" \n",
        "\n",
        "#immagini google\n",
        "path_img_google=\"/content/gdrive/MyDrive/immagini_google/\""
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dherpFOH0Uau"
      },
      "source": [
        "# **Collegamento google driv**e"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vyvTOQw-aHRP",
        "outputId": "67514ad6-2bc8-41ee-b684-0274352a4c0a"
      },
      "source": [
        "#collegamento google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5X10jetEyAax"
      },
      "source": [
        "# **Dataset**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eAtct4Tjyb0t"
      },
      "source": [
        "Load dataset stl10"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6hypnJdQu8lY"
      },
      "source": [
        "#10 classes: airplane, bird, car, cat, deer, dog, horse, monkey, ship, truck.\n",
        "#96x96 pixels, colored.\n",
        "(x_train, y_train), (x_test, y_test), min_, max_ = load_dataset(str(\"stl10\")) #carica train e test set"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ez_iiYZtu_-E",
        "outputId": "af9dffff-b764-4051-f316-4b6b64e436f4"
      },
      "source": [
        "print(x_train.shape,x_test.shape)\n",
        "#5000 immagini di train e 8000 di test: 500 train e 800 test per ogni classe"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(5000, 96, 96, 3) (8000, 96, 96, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mbwKfK6vwaKF"
      },
      "source": [
        "Riduzione training set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1kN7edZRAIwf"
      },
      "source": [
        "#x_train, y_train = x_train[:500], y_train[:500] #prende solo le prime 500 immagini di training\n",
        "x_test, y_test = x_test[:500], y_test[:500] #e le prime 10 di test"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HOY_tKeRyrTL"
      },
      "source": [
        "Stampa dimensioni"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QtNBe7ocyqyw",
        "outputId": "f28ecf20-033f-4e39-b3d5-ebc9c2de2614"
      },
      "source": [
        "im_shape = x_train[0].shape\n",
        "print(\"dimensioni immagine: \",im_shape)\n",
        "print(\"dimensioni train set: \",x_train.shape)\n",
        "print(\"dimensioni vettore classi reali: \",y_train.shape)\n",
        "print(\"dimensioni test set: \",x_test.shape)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dimensioni immagine:  (96, 96, 3)\n",
            "dimensioni train set:  (5000, 96, 96, 3)\n",
            "dimensioni vettore classi reali:  (5000, 10)\n",
            "dimensioni test set:  (500, 96, 96, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9XepXK-ny3XN"
      },
      "source": [
        "# **Creazione modello**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cf4-dqLky7m0"
      },
      "source": [
        "load vgg16 e freeze livelli inferiori"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GymaBXhaPMvL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e8ea86ea-b5e9-41a8-cba7-30bab7bcab70"
      },
      "source": [
        "#creazione rete usando vgg16 preaddestrata e aggiungendo gli ultimi livelli per adattarla al problema di 10 classi\n",
        "from tensorflow.keras.applications.vgg16 import VGG16\n",
        "\n",
        "#importa rete vgg16 addestrata sul dataset imagenet, esclude gli ultimi livelli e come input pongo dimensioni 96,96,3 \n",
        "vgg_conv = VGG16(weights='imagenet', include_top=False, input_shape=(im_shape[0],im_shape[1],im_shape[2]))\n",
        "\n",
        "# Freeze all the layers (non modifico i pesi dei livelli inferiori)\n",
        "for layer in vgg_conv.layers[:]:\n",
        "    layer.trainable = False"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "58892288/58889256 [==============================] - 1s 0us/step\n",
            "58900480/58889256 [==============================] - 1s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xyBgJAA6zJcI"
      },
      "source": [
        "Aggiunta livelli superiori"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9h_fBDmpzFa6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7d2a8ca6-4b5b-4cee-93f0-249753317a6e"
      },
      "source": [
        "# creo un modello aggiungendo livelli alla rete importata\n",
        "model = Sequential()\n",
        "# Add the vgg convolutional base model\n",
        "model.add(vgg_conv)\n",
        "# Add new layers\n",
        "model.add(Flatten())\n",
        "model.add(Dense(1024, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(10, activation='softmax')) #10: numero di valori in uscita (le classi)\n",
        "\n",
        "#stampa info del modello\n",
        "model.summary()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "vgg16 (Functional)           (None, 3, 3, 512)         14714688  \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 4608)              0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 1024)              4719616   \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 10)                10250     \n",
            "=================================================================\n",
            "Total params: 19,444,554\n",
            "Trainable params: 4,729,866\n",
            "Non-trainable params: 14,714,688\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l8z4jggt3ZUG"
      },
      "source": [
        "#Caricamento modello da drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bOb77pSE3cyB"
      },
      "source": [
        "#load model\n",
        "model= keras.models.load_model(path_model_save)"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZkxmNsf41B6Y"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YZko74RzzcCr"
      },
      "source": [
        "Configura parametri modello"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vfXtu3eazk5l"
      },
      "source": [
        "# Configure the model for training (setta i parametri del modello)\n",
        "import keras\n",
        "model.compile(\n",
        "        loss=keras.losses.categorical_crossentropy, optimizer=Adam(learning_rate=1e-3), metrics=[\"accuracy\"]\n",
        "    )"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X-b3YpaWznuq"
      },
      "source": [
        "Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tr5rYORYzNxu",
        "outputId": "ccb8f855-f1da-43d1-ac9f-7dc57b54d156"
      },
      "source": [
        "# Train the model: divisione del training set in training e validation con rapporto 80-20\n",
        "history= model.fit(x_train,y_train,epochs=25,batch_size=128)#,validation_split=0.2)\n",
        "#batch 64, lr -5, 35 epoche = 70%\n",
        "#batch 128, lr -3, 20 epoche = 98% --- test set 500 elementi: 69%, re-training senza validation acc test=69%\n",
        "#batch 128, lr -3, 25 epoche, senza validation = 98% ---test set 500 elementi 70%\n",
        "#batch 128, lr -4, 20 epoche = 92%\n",
        "#batch 64, lr -4, 20 epoche = 95%"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train on 5000 samples\n",
            "Epoch 1/25\n",
            "5000/5000 [==============================] - 46s 9ms/sample - loss: 1.4283 - accuracy: 0.5108\n",
            "Epoch 2/25\n",
            "5000/5000 [==============================] - 8s 2ms/sample - loss: 0.9028 - accuracy: 0.6664\n",
            "Epoch 3/25\n",
            "5000/5000 [==============================] - 8s 2ms/sample - loss: 0.7625 - accuracy: 0.7336\n",
            "Epoch 4/25\n",
            "5000/5000 [==============================] - 8s 2ms/sample - loss: 0.6614 - accuracy: 0.7634\n",
            "Epoch 5/25\n",
            "5000/5000 [==============================] - 8s 2ms/sample - loss: 0.5640 - accuracy: 0.8028\n",
            "Epoch 6/25\n",
            "5000/5000 [==============================] - 8s 2ms/sample - loss: 0.5263 - accuracy: 0.8100\n",
            "Epoch 7/25\n",
            "5000/5000 [==============================] - 8s 2ms/sample - loss: 0.4368 - accuracy: 0.8492\n",
            "Epoch 8/25\n",
            "5000/5000 [==============================] - 8s 2ms/sample - loss: 0.3907 - accuracy: 0.8582\n",
            "Epoch 9/25\n",
            "5000/5000 [==============================] - 8s 2ms/sample - loss: 0.3380 - accuracy: 0.8870\n",
            "Epoch 10/25\n",
            "5000/5000 [==============================] - 8s 2ms/sample - loss: 0.3409 - accuracy: 0.8830\n",
            "Epoch 11/25\n",
            "5000/5000 [==============================] - 8s 2ms/sample - loss: 0.2821 - accuracy: 0.9044\n",
            "Epoch 12/25\n",
            "5000/5000 [==============================] - 8s 2ms/sample - loss: 0.2870 - accuracy: 0.9038\n",
            "Epoch 13/25\n",
            "5000/5000 [==============================] - 8s 2ms/sample - loss: 0.2537 - accuracy: 0.9182\n",
            "Epoch 14/25\n",
            "5000/5000 [==============================] - 8s 2ms/sample - loss: 0.2249 - accuracy: 0.9258\n",
            "Epoch 15/25\n",
            "5000/5000 [==============================] - 8s 2ms/sample - loss: 0.2046 - accuracy: 0.9326\n",
            "Epoch 16/25\n",
            "5000/5000 [==============================] - 8s 2ms/sample - loss: 0.1712 - accuracy: 0.9450\n",
            "Epoch 17/25\n",
            "5000/5000 [==============================] - 8s 2ms/sample - loss: 0.1722 - accuracy: 0.9430\n",
            "Epoch 18/25\n",
            "5000/5000 [==============================] - 8s 2ms/sample - loss: 0.1485 - accuracy: 0.9516\n",
            "Epoch 19/25\n",
            "5000/5000 [==============================] - 8s 2ms/sample - loss: 0.1432 - accuracy: 0.9566\n",
            "Epoch 20/25\n",
            "5000/5000 [==============================] - 8s 2ms/sample - loss: 0.1249 - accuracy: 0.9590\n",
            "Epoch 21/25\n",
            "5000/5000 [==============================] - 8s 2ms/sample - loss: 0.1127 - accuracy: 0.9666\n",
            "Epoch 22/25\n",
            "5000/5000 [==============================] - 8s 2ms/sample - loss: 0.1030 - accuracy: 0.9718\n",
            "Epoch 23/25\n",
            "5000/5000 [==============================] - 8s 2ms/sample - loss: 0.0868 - accuracy: 0.9782\n",
            "Epoch 24/25\n",
            "5000/5000 [==============================] - 8s 2ms/sample - loss: 0.0904 - accuracy: 0.9726\n",
            "Epoch 25/25\n",
            "5000/5000 [==============================] - 8s 2ms/sample - loss: 0.0717 - accuracy: 0.9824\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tFFWkmjX2j5P"
      },
      "source": [
        "#Salvataggio modello"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TAie77UO2nTs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b752edf4-c20b-4fc4-aa53-afbde080dcb0"
      },
      "source": [
        "#salva modello su drive\n",
        "model.save(path_model_save)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Assets written to: /content/gdrive/My Drive/ModelliCNN/model_prova.pt/assets\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VjnPQ4NtzzQH"
      },
      "source": [
        "# **Creazione classificatore ART**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OWZ_zpbiZjyd"
      },
      "source": [
        "#creazione classificatore di tipo Keras usando il modello addestrato, poiché ART supporta solo determinati classificatori\n",
        "classifier = KerasClassifier(model=model, clip_values=(min_, max_)) #è un wrapper messo a disposizione da ART per creare un classificatore a partire da un modello addestrato"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_UByTbhH01i3"
      },
      "source": [
        "# Esempio resize"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GyuRBEcgsiN7"
      },
      "source": [
        "#resized=cv2.resize(x_test1[1,:,:,:],(500,500),interpolation=cv2.INTER_CUBIC)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4X4ZaKM8z9hk"
      },
      "source": [
        "# **Testing su immagini originali**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vvPxa40rAgpa",
        "outputId": "7529f02e-3e11-422a-cfdd-5f7c27715bfa"
      },
      "source": [
        "# Evaluate the classifier on the test set \n",
        "value_preds=classifier.predict(x_test) #contiene i valori tra 0 e 1 predetti per ognuna delle 10 classi e per ogni immagine\n",
        "preds = np.argmax(value_preds, axis=1) #(le predizioni vanno da 0 a 9 e indicano la classe predetta)\n",
        "acc = np.sum(preds == np.argmax(y_test, axis=1)) / y_test.shape[0]\n",
        "print(\"Accuracy on test set:\", (acc * 100))\n",
        "#print(\"classi predette: \",preds)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/engine/training.py:2470: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on test set: 70.6\n",
            "classi predette:  [6 7 5 0 5 1 1 1 1 3 0 0 3 0 7 3 4 9 0 6 6 6 1 3 2 2 5 5 2 1 0 6 7 7 9 2 3\n",
            " 7 9 1 1 9 0 5 4 9 1 1 3 2 5 7 6 0 8 8 6 6 2 9 0 3 5 7 9 6 1 8 0 7 2 1 7 4\n",
            " 2 7 3 3 5 7 5 0 9 3 8 1 3 0 8 7 9 3 3 6 5 1 7 6 8 5 8 3 0 2 8 7 1 2 3 4 3\n",
            " 0 3 7 0 0 3 4 9 1 5 1 8 3 2 7 8 5 1 3 6 8 7 7 1 3 0 1 5 7 1 7 1 1 4 3 9 9\n",
            " 0 1 3 8 7 4 0 3 4 1 2 8 1 5 5 5 5 8 9 4 0 6 6 1 4 4 9 2 3 3 1 2 1 5 8 7 8\n",
            " 7 7 2 6 9 5 9 2 8 8 4 8 9 1 5 2 7 8 7 3 5 4 4 2 5 9 4 8 3 4 1 2 5 1 1 0 0\n",
            " 1 4 2 1 4 3 9 8 7 2 9 1 6 1 4 8 5 9 8 0 8 9 0 5 1 6 0 4 0 2 9 5 7 6 3 6 5\n",
            " 7 1 1 1 9 2 5 9 2 5 7 8 2 1 8 1 1 9 0 8 0 4 1 8 0 0 2 9 0 9 1 8 1 7 3 3 3\n",
            " 4 9 9 4 8 9 3 9 3 5 0 1 4 5 1 4 1 2 5 0 5 2 8 5 9 3 5 5 3 7 6 6 0 9 1 1 2\n",
            " 5 8 1 1 6 9 0 6 2 6 9 4 1 3 9 8 1 6 1 9 7 1 5 6 7 2 8 6 1 8 5 1 3 8 5 1 2\n",
            " 5 9 5 4 8 9 9 1 5 1 9 2 1 4 1 3 3 1 8 0 9 4 0 4 3 5 4 0 1 0 1 5 9 5 2 0 9\n",
            " 2 7 2 1 7 1 8 0 7 5 2 0 9 1 1 0 1 2 3 2 5 5 9 8 6 6 4 3 3 1 2 2 2 8 8 0 4\n",
            " 0 4 5 1 9 6 1 7 8 8 6 0 0 5 1 2 8 6 5 1 7 4 5 9 7 1 2 9 4 1 1 6 5 3 6 3 0\n",
            " 5 0 1 2 2 8 3 5 5 4 0 8 7 9 7 1 9 6 6]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Suibji7V0I7j"
      },
      "source": [
        "# **Attacco**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fq6B6unj2HcX"
      },
      "source": [
        "Definizione attacco"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DZ-ghSjs2Ma0"
      },
      "source": [
        "# FGM\n",
        "attack = FastGradientMethod(estimator=classifier, eps=0.1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cYPmQSaL2Oxa"
      },
      "source": [
        "Generazione adversarial samples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gl_FoOil2SrO"
      },
      "source": [
        "x_test_adv = attack.generate(x=x_test,y=y_test) #aggiunge una perturbazione alle immagini del test set\n",
        "#N.B. si possono passare le classi reali del test set (con y=y_test) e in questo caso FGM calcolerà le perturbazioni in modo che il classificatore\n",
        "#non predica queste classi. Mentre, se non passo y_test, le calcolerà in modo da non fargli predire la classe predetta normalmente (la quale può essere diversa da quella reale\n",
        "#se il classificatore non è corretto)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P3kr1yAn2UBl"
      },
      "source": [
        "Calcolo perturbazioni"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MgPraX51Z15a"
      },
      "source": [
        "perturb=x_test_adv-x_test #calcolo della perturbazione\n",
        "\n",
        "#N.B. problema nel calcolo della perturbazione, con la sottrazione si ha un'immagine [-1,1], di seguito è normalizzata in [0,1]\n",
        "min_p, max_p = np.amin(perturb), np.amax(perturb)\n",
        "perturb = (perturb - min_p) / (max_p - min_p)\n",
        "#se non normalizzo, la funzione che uso per stampare ritaglia automaticamente nell'intervallo [0,1] quindi perdo informazioni"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nH0_wJWt2XuF"
      },
      "source": [
        "#**Testing su immagini perturbate**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V00SgiXfaAFl",
        "outputId": "beac1a52-a4bd-4cfc-9930-9b1678fd1a45"
      },
      "source": [
        "# Evaluate the classifier on the adversarial samples FGM\n",
        "value_preds_after=classifier.predict(x_test_adv)\n",
        "preds_after = np.argmax(value_preds_after, axis=1)\n",
        "acc_after = np.sum(preds_after == np.argmax(y_test, axis=1)) / y_test.shape[0]\n",
        "print(\"Accuracy on adversarial samples:\", (acc_after * 100))\n",
        "print(\"classi predette su test set perturbato: \",preds_after)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on adversarial samples: 0.0\n",
            "classi predette su test set perturbato:  [4 3 3 3 5 6 6 4 5 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nLcnge6A0Npi"
      },
      "source": [
        "# **Stampa subplot**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cB1Q6Id6_CNR"
      },
      "source": [
        "#stampa subplot\n",
        "\n",
        "#cv2_imshow(cv2.hconcat([x_test[0,:,:,:]*255,x_test[1,:,:,:]*255,x_test[2,:,:,:]*255]))\n",
        "#N.B. plt.imshow usa range 0,1 per immagini rgb, mentre cv2 usa 0,255\n",
        "import matplotlib.pyplot as plt    \n",
        "#nel seguente ciclo for si crea un vettore delle classi predette ordinato per probabilità decrescente\n",
        "for i in range(x_test.shape[0]):\n",
        "  value=value_preds_after[i,:]\n",
        "  value_sorted=sorted(value,reverse=True)\n",
        "  classes=[]\n",
        "  for j in range(value.size) :\n",
        "    ind=np.where(value==value_sorted[j]) #restituisce l'indice in value del valore uguale a value_sorted[i], quindi è la classe\n",
        "    classes.append(ind[0][0]) #classes è il vettore finale\n",
        "  \n",
        "  #in seguito per ogni immagine del test set si stampa un subplot\n",
        "  fig = plt.figure()\n",
        "  print(\"Immagine \",i)\n",
        "  #originale\n",
        "  ax1 = fig.add_subplot(321) #subplot con 3 righe e due colonne\n",
        "  ax1.axis('off')\n",
        "  ax1.imshow(cv2.rotate(x_test[i,:,:,:],cv2.cv2.ROTATE_90_CLOCKWISE))\n",
        "  ax1.title.set_text(\"originale\\nclasse reale: \"+str(np.argmax(y_test[i,:]))+\"\\nclasse predetta: \"+str(preds[i]))\n",
        "  #perturbazione\n",
        "  ax2 = fig.add_subplot(322)\n",
        "  ax2.imshow(cv2.rotate(perturb[i,:,:,:],cv2.cv2.ROTATE_90_CLOCKWISE))\n",
        "  ax2.axis('off')\n",
        "  ax2.title.set_text(\"perturbazione\")\n",
        "  #perturbata\n",
        "  ax3 = fig.add_subplot(325)\n",
        "  ax3.imshow(cv2.rotate(x_test_adv[i,:,:,:],cv2.cv2.ROTATE_90_CLOCKWISE))\n",
        "  ax3.axis('off')\n",
        "  ax3.title.set_text(\"perturbata\\ntop5 classi predette: \"+str(classes[:5])+\"\\ncon i valori: \"+str(value_sorted[:5]))\n",
        "\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nkdhgkab0e-B"
      },
      "source": [
        "# Def funzione che perturba una singola immagine"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jkTzu6ZAcbgU"
      },
      "source": [
        "def perturb_img(img_g : np.ndarray,real_class : int):\n",
        "  img_g=cv2.resize(img_g,(96,96))\n",
        "  print(\"\\nImmagine originale, classe reale: \",real_class)\n",
        "  cv2_imshow(img_g)\n",
        "\n",
        "  min_, max_ = np.amin(img_g), np.amax(img_g)\n",
        "  normalized_img_g = (img_g - min_) / (max_ - min_) #normalizzo tra [0,1]\n",
        "  normalized_img_g=normalized_img_g.reshape(1,96,96,3)\n",
        "\n",
        "  pred_value=classifier.predict(normalized_img_g)\n",
        "  pred=np.argmax(pred_value)\n",
        "  print(\"Classe predetta: \",pred)\n",
        "  print(\"Valori predetti per ogni classe:\\n\",pred_value)\n",
        "  #generazione perturbazione\n",
        "  img_adv = attack.generate(x=normalized_img_g)\n",
        "  print(\"\\nImmagine perturbata\")\n",
        "  cv2_imshow(img_adv[0,:,:,:]*max_)\n",
        "  pred_value_adv=classifier.predict(img_adv)\n",
        "  pred_adv=np.argmax(pred_value_adv)\n",
        "  print(\"Classe predetta: \",pred_adv)\n",
        "  print(\"Valori predetti per ogni classe:\\n\",pred_value_adv)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fHJP8DBX0k90"
      },
      "source": [
        "# Testing su immagine di google"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TcVXbpP3TBk8"
      },
      "source": [
        "#test su immagini di google\n",
        "\n",
        "#10 classes: airplane, bird, car, cat, deer, dog, horse, monkey, ship, truck.\n",
        "\n",
        "img_g1=cv2.imread(path_img_drive+\"airplane.jpg\")\n",
        "img_g2=cv2.imread(path_img_drive+\"dog.jpg\")\n",
        "\n",
        "perturb_img(img_g1,0)\n",
        "perturb_img(img_g2,5)\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}